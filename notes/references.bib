@inproceedings {singh2007mapper,
    booktitle = {Eurographics Symposium on Point-Based Graphics},
    editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker},
    title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition}},
    author = {Singh, Gurjeet and Memoli, Facundo and Carlsson, Gunnar},
    year = {2007},
    publisher = {The Eurographics Association},
    ISSN = {1811-7813},
    ISBN = {978-3-905673-51-7},
    DOI = {10.2312/SPBG/SPBG07/091-100}
}

@article{JMLR:v9:vandermaaten08a,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@book{jurafsky2009speech,
    author={Daniel Jurafsky and James H. Martin},
    title={Speech and Language Processing},
    year={2009},
    ISBN={978-0-13-187321-6},
    publisher={MIT Press}
}

@inproceedings{DBLP:journals/corr/abs-1301-3781,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MikolovSCCD13,
  author    = {Tom{\'{a}}s Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  eprinttype = {arXiv},
  eprint    = {1310.4546},
  timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MikolovSCCD13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MikolovLS13,
  author    = {Tom{\'{a}}s Mikolov and
               Quoc V. Le and
               Ilya Sutskever},
  title     = {Exploiting Similarities among Languages for Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1309.4168},
  year      = {2013},
  url       = {http://arxiv.org/abs/1309.4168},
  eprinttype = {arXiv},
  eprint    = {1309.4168},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/MikolovLS13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Rong14,
  author    = {Xin Rong},
  title     = {word2vec Parameter Learning Explained},
  journal   = {CoRR},
  volume    = {abs/1411.2738},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.2738},
  eprinttype = {arXiv},
  eprint    = {1411.2738},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Rong14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2014_feab05aa,
	author = {Levy, Omer and Goldberg, Yoav},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Neural Word Embedding as Implicit Matrix Factorization},
	url = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf}}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@article{levy-etal-2015-improving,
    title = "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
    author = "Levy, Omer  and
      Goldberg, Yoav  and
      Dagan, Ido",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1016",
    doi = "10.1162/tacl_a_00134",
    pages = "211--225",
    abstract = "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.",
}

@article{DBLP:journals/corr/Goldberg15c,
  author    = {Yoav Goldberg},
  title     = {A Primer on Neural Network Models for Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1510.00726},
  year      = {2015},
  url       = {http://arxiv.org/abs/1510.00726},
  eprinttype = {arXiv},
  eprint    = {1510.00726},
  timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Goldberg15c.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BolukbasiCZSK16a,
  author    = {Tolga Bolukbasi and
               Kai{-}Wei Chang and
               James Y. Zou and
               Venkatesh Saligrama and
               Adam Kalai},
  title     = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing
               Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1607.06520},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06520},
  eprinttype = {arXiv},
  eprint    = {1607.06520},
  timestamp = {Mon, 13 Aug 2018 16:46:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BolukbasiCZSK16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hamilton-etal-2016-diachronic,
    title = "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
    author = "Hamilton, William L.  and
      Leskovec, Jure  and
      Jurafsky, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1141",
    doi = "10.18653/v1/P16-1141",
    pages = "1489--1501",
}

@article{DBLP:journals/corr/BojanowskiGJM16,
  author    = {Piotr Bojanowski and
               Edouard Grave and
               Armand Joulin and
               Tom{\'{a}}s Mikolov},
  title     = {Enriching Word Vectors with Subword Information},
  journal   = {CoRR},
  volume    = {abs/1607.04606},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.04606},
  eprinttype = {arXiv},
  eprint    = {1607.04606},
  timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wu2016google,
	title={Google's neural machine translation system: Bridging the gap between human and machine translation},
	author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
	journal={arXiv preprint arXiv:1609.08144},
	year={2016}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1710-04087,
  author    = {Alexis Conneau and
               Guillaume Lample and
               Marc'Aurelio Ranzato and
               Ludovic Denoyer and
               Herv{\'{e}} J{\'{e}}gou},
  title     = {Word Translation Without Parallel Data},
  journal   = {CoRR},
  volume    = {abs/1710.04087},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.04087},
  eprinttype = {arXiv},
  eprint    = {1710.04087},
  timestamp = {Mon, 13 Aug 2018 16:48:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-04087.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mimno-thompson-2017-strange,
    title = "The strange geometry of skip-gram with negative sampling",
    author = "Mimno, David  and
      Thompson, Laure",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1308",
    doi = "10.18653/v1/D17-1308",
    pages = "2873--2878",
    abstract = "Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.",
}

@article{DBLP:journals/corr/MuBV17,
  author    = {Jiaqi Mu and
               Suma Bhat and
               Pramod Viswanath},
  title     = {All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
  journal   = {CoRR},
  volume    = {abs/1702.01417},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.01417},
  eprinttype = {arXiv},
  eprint    = {1702.01417},
  timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MuBV17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yu-etal-2017-refining,
    title = "Refining Word Embeddings for Sentiment Analysis",
    author = "Yu, Liang-Chih  and
      Wang, Jin  and
      Lai, K. Robert  and
      Zhang, Xuejie",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1056",
    doi = "10.18653/v1/D17-1056",
    pages = "534--539",
    abstract = "Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).",
}

@article{DBLP:journals/corr/NickelK17,
  author    = {Maximilian Nickel and
               Douwe Kiela},
  title     = {Poincar{\'{e}} Embeddings for Learning Hierarchical Representations},
  journal   = {CoRR},
  volume    = {abs/1705.08039},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.08039},
  eprinttype = {arXiv},
  eprint    = {1705.08039},
  timestamp = {Mon, 13 Aug 2018 16:47:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NickelK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nakashole-flauger-2018-characterizing,
    title = "Characterizing Departures from Linearity in Word Translation",
    author = "Nakashole, Ndapa  and
      Flauger, Raphael",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2036",
    doi = "10.18653/v1/P18-2036",
    pages = "221--227",
    abstract = "We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",
}

@article{DBLP:journals/corr/abs-1808-08933,
  author    = {Xilun Chen and
               Claire Cardie},
  title     = {Unsupervised Multilingual Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1808.08933},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.08933},
  eprinttype = {arXiv},
  eprint    = {1808.08933},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-08933.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{budzianowski-etal-2018-multiwoz,
    title = "{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling",
    author = "Budzianowski, Pawe{\l}  and
      Wen, Tsung-Hsien  and
      Tseng, Bo-Hsiang  and
      Casanueva, I{\~n}igo  and
      Ultes, Stefan  and
      Ramadan, Osman  and
      Ga{\v{s}}i{\'c}, Milica",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = "10",
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1547",
    doi = "10.18653/v1/D18-1547",
    pages = "5016--5026",
    abstract = "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
    eprint={1810.00278},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{rastogi2020towards,
	title={Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset},
	author={Rastogi, Abhinav and Zang, Xiaoxue and Sunkara, Srinivas and Gupta, Raghav and Khaitan, Pranav},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={34},
	number={05},
	pages={8689--8696},
	year={2020}
}

@article{DBLP:journals/corr/abs-1908-10084,
  author    = {Nils Reimers and
               Iryna Gurevych},
  title     = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  journal   = {CoRR},
  volume    = {abs/1908.10084},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.10084},
  eprinttype = {arXiv},
  eprint    = {1908.10084},
  timestamp = {Thu, 26 Nov 2020 12:13:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10084.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1904-02239,
  author    = {Valentin Khrulkov and
               Leyla Mirvakhabova and
               Evgeniya Ustinova and
               Ivan V. Oseledets and
               Victor S. Lempitsky},
  title     = {Hyperbolic Image Embeddings},
  journal   = {CoRR},
  volume    = {abs/1904.02239},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.02239},
  eprinttype = {arXiv},
  eprint    = {1904.02239},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-02239.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-06546,
  author    = {Alexandru Tifrea and
               Gary B{\'{e}}cigneul and
               Octavian{-}Eugen Ganea},
  title     = {Poincar{\'{e}} GloVe: Hyperbolic Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1810.06546},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.06546},
  eprinttype = {arXiv},
  eprint    = {1810.06546},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-06546.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-04313,
  author    = {Bhuwan Dhingra and
               Christopher J. Shallue and
               Mohammad Norouzi and
               Andrew M. Dai and
               George E. Dahl},
  title     = {Embedding Text in Hyperbolic Spaces},
  journal   = {CoRR},
  volume    = {abs/1806.04313},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.04313},
  eprinttype = {arXiv},
  eprint    = {1806.04313},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-04313.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1802-05365,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Mohit Iyyer and
               Matt Gardner and
               Christopher Clark and
               Kenton Lee and
               Luke Zettlemoyer},
  title     = {Deep contextualized word representations},
  journal   = {CoRR},
  volume    = {abs/1802.05365},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05365},
  eprinttype = {arXiv},
  eprint    = {1802.05365},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{eisenstein2019introduction,
  title={Introduction to Natural Language Processing},
  author={Eisenstein, J.},
  isbn={9780262042840},
  lccn={2018059552},
  series={Adaptive Computation and Machine Learning series},
  url={https://books.google.de/books?id=72yuDwAAQBAJ},
  year={2019},
  publisher={MIT Press}
}

@article{DBLP:journals/corr/abs-1901-09813,
  author    = {Carl Allen and
               Timothy M. Hospedales},
  title     = {Analogies Explained: Towards Understanding Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1901.09813},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.09813},
  eprinttype = {arXiv},
  eprint    = {1901.09813},
  timestamp = {Sat, 23 Jan 2021 01:12:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09813.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{murugan2019introduction,
      title={An Introduction to Topological Data Analysis for Physicists: From LGM to FRBs}, 
      author={Jeff Murugan and Duncan Robertson},
      year={2019},
      eprint={1904.11044},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM}
}

@inproceedings{ethayarajh-2019-contextual,
    title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
    author = "Ethayarajh, Kawin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1006",
    doi = "10.18653/v1/D19-1006",
    pages = "55--65",
    abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
}

@article{sentiment-aware-word-embedding-emotion-2019, 
    title={Sentiment-Aware Word Embedding for Emotion Classification},
    author={Mao, Xingliang and Chang, Shuai and Shi, Jinjing and Li, Fangfang and Shi, Ronghua},
    volume={9},
    ISSN={2076-3417},
    url={http://dx.doi.org/10.3390/app9071334},
    DOI={10.3390/app9071334},
    number={7},
    journal={Applied Sciences},
    publisher={MDPI AG},
    year={2019},
    pages={1334}
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@inproceedings{zhao-etal-2019-gender,
    title = "Gender Bias in Contextualized Word Embeddings",
    author = "Zhao, Jieyu  and
      Wang, Tianlu  and
      Yatskar, Mark  and
      Cotterell, Ryan  and
      Ordonez, Vicente  and
      Chang, Kai-Wei",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1064",
    doi = "10.18653/v1/N19-1064",
    pages = "629--634",
    abstract = "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo{'}s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",
}

@inproceedings{kurita-etal-2019-measuring,
    title = "Measuring Bias in Contextualized Word Representations",
    author = "Kurita, Keita  and
      Vyas, Nidhi  and
      Pareek, Ayush  and
      Black, Alan W  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3823",
    doi = "10.18653/v1/W19-3823",
    pages = "166--172",
    abstract = "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
}

@inproceedings{garg-etal-2019-jointly,
    title = "Jointly Learning to Align and Translate with Transformer Models",
    author = "Garg, Sarthak  and
      Peitz, Stephan  and
      Nallasamy, Udhyakumar  and
      Paulik, Matthias",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1453",
    doi = "10.18653/v1/D19-1453",
    pages = "4453--4462",
    abstract = "The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets.",
}

@inproceedings{li-etal-2020-sentence,
    title = "On the Sentence Embeddings from Pre-trained Language Models",
    author = "Li, Bohan  and
      Zhou, Hao  and
      He, Junxian  and
      Wang, Mingxuan  and
      Yang, Yiming  and
      Li, Lei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.733",
    doi = "10.18653/v1/2020.emnlp-main.733",
    pages = "9119--9130",
    abstract = "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \url{https://github.com/bohanli/BERT-flow}.",
}

@inproceedings{jakubowski2020topology,
     title={Topology of word embeddings: Singularities reflect polysemy},
    author={Jakubowski, Alexander and Gasic, Milica and Zibrowius, Marcus},
    booktitle={Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics},
    pages={103--113},
    year={2020},
    eprint={2011.09413},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{ravfogel-etal-2020-null,
    title = "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
    author = "Ravfogel, Shauli  and
      Elazar, Yanai  and
      Gonen, Hila  and
      Twiton, Michael  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.647",
    doi = "10.18653/v1/2020.acl-main.647",
    pages = "7237--7256",
    abstract = "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
}

@inproceedings{gonen-etal-2020-simple,
    title = "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora",
    author = "Gonen, Hila  and
      Jawahar, Ganesh  and
      Seddah, Djam{\'e}  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.51",
    doi = "10.18653/v1/2020.acl-main.51",
    pages = "538--555",
    abstract = "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",
}

@inproceedings{zhu-etal-2020-hypertext,
    title = "{H}yper{T}ext: Endowing {F}ast{T}ext with Hyperbolic Geometry",
    author = "Zhu, Yudong  and
      Zhou, Di  and
      Xiao, Jinghui  and
      Jiang, Xin  and
      Chen, Xiao  and
      Liu, Qun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.104",
    doi = "10.18653/v1/2020.findings-emnlp.104",
    pages = "1166--1171",
    abstract = "Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchies precisely with limited representation capacity. Considering that hyperbolic space is naturally suitable for modelling tree-like hierarchical data, we propose a new model named HyperText for efficient text classification by endowing FastText with hyperbolic geometry. Empirically, we show that HyperText outperforms FastText on a range of text classification tasks with much reduced parameters.",
}


@misc{reinauer2021persformer,
    title={Persformer: A Transformer Architecture for Topological Machine Learning}, 
    author={Raphael Reinauer and Matteo Caorsi and Nicolas Berkouk},
    year={2021},
    eprint={2112.15210},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2007-01852,
  author    = {Fangxiaoyu Feng and
               Yinfei Yang and
               Daniel Cer and
               Naveen Arivazhagan and
               Wei Wang},
  title     = {Language-agnostic {BERT} Sentence Embedding},
  journal   = {CoRR},
  volume    = {abs/2007.01852},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.01852},
  eprinttype = {arXiv},
  eprint    = {2007.01852},
  timestamp = {Mon, 06 Jul 2020 15:26:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-01852.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{guhr-etal-2020-training,
    title = "Training a Broad-Coverage {G}erman Sentiment Classification Model for Dialog Systems",
    author = {Guhr, Oliver  and
      Schumann, Anne-Kathrin  and
      Bahrmann, Frank  and
      B{\"o}hme, Hans Joachim},
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.202",
    pages = "1627--1632",
    abstract = "This paper describes the training of a general-purpose German sentiment classification model. Sentiment classification is an important aspect of general text analytics. Furthermore, it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. The presented study outlines how we have collected a new German sentiment corpus and then combined this corpus with existing resources to train a broad-coverage German sentiment model. The resulting data set contains 5.4 million labelled samples. We have used the data to train both, a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. The model and the data set will be published along with this paper.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{barbieri-etal-2020-tweeteval,
    title = "{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification",
    author = "Barbieri, Francesco  and
      Camacho-Collados, Jose  and
      Espinosa Anke, Luis  and
      Neves, Leonardo",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.148",
    doi = "10.18653/v1/2020.findings-emnlp.148",
    pages = "1644--1650",
    abstract = "The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.",
}

@article{DBLP:journals/corr/abs-2104-03869,
  author    = {Boli Chen and
               Yao Fu and
               Guangwei Xu and
               Pengjun Xie and
               Chuanqi Tan and
               Mosha Chen and
               Liping Jing},
  title     = {Probing {BERT} in Hyperbolic Spaces},
  journal   = {CoRR},
  volume    = {abs/2104.03869},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.03869},
  eprinttype = {arXiv},
  eprint    = {2104.03869},
  timestamp = {Tue, 13 Apr 2021 16:46:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-03869.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dey2021computational,
    title={Computational Topology for Data Analysis},
    author={Dey, Tamal K and Wang, Yusu},
    year={2021},
    publisher={Cambridge University Press},
    notes={\url{https://www.cs.purdue.edu/homes/tamaldey/book/CTDAbook/CTDAbook.pdf}}
}

@misc{vanniekerk2021uncertainty,
      title={Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance}, 
      author={Carel van Niekerk and Andrey Malinin and Christian Geishauser and Michael Heck and Hsien-chin Lin and Nurul Lubis and Shutong Feng and Milica Gašić},
      year={2021},
      eprint={2109.04349},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kumar2021identifying,
      title={Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings}, 
      author={Vaibhav Kumar and Tenzin Singhay Bhotia and Vaibhav Kumar and Tanmoy Chakraborty},
      year={2021},
      eprint={2109.13767},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lupo2021persistence,
      title={Persistence Steenrod modules}, 
      author={Umberto Lupo and Anibal M. Medina-Mardones and Guillaume Tauzin},
      year={2021},
      eprint={1812.05031},
      archivePrefix={arXiv},
      primaryClass={math.AT}
}

@article{DBLP:journals/corr/abs-2109-04919,
  author    = {Shutong Feng and
               Nurul Lubis and
               Christian Geishauser and
               Hsien{-}Chin Lin and
               Michael Heck and
               Carel van Niekerk and
               Milica Gasic},
  title     = {EmoWOZ: {A} Large-Scale Corpus and Labelling Scheme for Emotion in
               Task-Oriented Dialogue Systems},
  journal   = {CoRR},
  volume    = {abs/2109.04919},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.04919},
  eprinttype = {arXiv},
  eprint    = {2109.04919},
  timestamp = {Thu, 30 Dec 2021 14:34:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-04919.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2109-13059,
  author    = {Fangyu Liu and
               Yunlong Jiao and
               Jordan Massiah and
               Emine Yilmaz and
               Serhii Havrylov},
  title     = {Trans-Encoder: Unsupervised sentence-pair modelling through self-
               and mutual-distillations},
  journal   = {CoRR},
  volume    = {abs/2109.13059},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.13059},
  eprinttype = {arXiv},
  eprint    = {2109.13059},
  timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-13059.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2104-08821,
  author    = {Tianyu Gao and
               Xingcheng Yao and
               Danqi Chen},
  title     = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
  journal   = {CoRR},
  volume    = {abs/2104.08821},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08821},
  eprinttype = {arXiv},
  eprint    = {2104.08821},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08821.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{kaneko-bollegala-2021-debiasing,
    title = "Debiasing Pre-trained Contextualised Embeddings",
    author = "Kaneko, Masahiro and Bollegala, Danushka",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.107",
    doi = "10.18653/v1/2021.eacl-main.107",
    pages = "1256--1266",
    abstract = "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",
}

@InProceedings{pmlr-v139-schlag21a,
	title = 	 {Linear Transformers Are Secretly Fast Weight Programmers},
	author =       {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {9355--9366},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {6},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf},
	url = 	 {https://proceedings.mlr.press/v139/schlag21a.html},
	abstract = 	 {We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early ’90s, where a slow neural net learns by gradient descent to program the fast weights of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.}
}

@article{DBLP:journals/corr/abs-2111-07180,
	author    = {Yizhen Zhang and
	Minkyu Choi and
	Kuan Han and
	Zhongming Liu},
	title     = {Explainable Semantic Space by Grounding Language to Vision with Cross-Modal
	Contrastive Learning},
	journal   = {CoRR},
	volume    = {abs/2111.07180},
	year      = {2021},
	url       = {https://arxiv.org/abs/2111.07180},
	eprinttype = {arXiv},
	eprint    = {2111.07180},
	timestamp = {Tue, 16 Nov 2021 12:12:31 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2111-07180.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2205.13568,
  doi = {10.48550/ARXIV.2205.13568},
  url = {https://arxiv.org/abs/2205.13568},
  author = {Zhou, Zhihan and Zhang, Dejiao and Xiao, Wei and Dingwall, Nicholas and Ma, Xiaofei and Arnold, Andrew O. and Xiang, Bing},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Dialogue Representations from Consecutive Utterances},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


@article{DBLP:journals/corr/abs-2104-02797,
  author    = {Archit Rathore and
               Sunipa Dev and
               Jeff M. Phillips and
               Vivek Srikumar and
               Yan Zheng and
               Chin{-}Chia Michael Yeh and
               Junpeng Wang and
               Wei Zhang and
               Bei Wang},
  title     = {{VERB:} Visualizing and Interpreting Bias Mitigation Techniques for
               Word Representations},
  journal   = {CoRR},
  volume    = {abs/2104.02797},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.02797},
  eprinttype = {arXiv},
  eprint    = {2104.02797},
  timestamp = {Fri, 02 Jul 2021 16:07:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-02797.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/naacl/PetersNIGCLZ18,
	author    = {Matthew E. Peters and
	Mark Neumann and
	Mohit Iyyer and
	Matt Gardner and
	Christopher Clark and
	Kenton Lee and
	Luke Zettlemoyer},
	editor    = {Marilyn A. Walker and
	Heng Ji and
	Amanda Stent},
	title     = {Deep Contextualized Word Representations},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of
	the Association for Computational Linguistics: Human Language Technologies,
	{NAACL-HLT} 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume
	1 (Long Papers)},
	pages     = {2227--2237},
	publisher = {Association for Computational Linguistics},
	year      = {2018},
	url       = {https://doi.org/10.18653/v1/n18-1202},
	doi       = {10.18653/v1/n18-1202},
	timestamp = {Fri, 06 Aug 2021 00:41:32 +0200},
	biburl    = {https://dblp.org/rec/conf/naacl/PetersNIGCLZ18.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{sutskever2014sequence,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	journal={Advances in neural information processing systems},
	volume={27},
	year={2014}
}

@inproceedings{DBLP:journals/corr/BahdanauCB14,
	author    = {Dzmitry Bahdanau and
	Kyunghyun Cho and
	Yoshua Bengio},
	editor    = {Yoshua Bengio and
	Yann LeCun},
	title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
	San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	year      = {2015},
	url       = {http://arxiv.org/abs/1409.0473},
	timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{voitseq2seq,
	title = {Sequence to Sequence (seq2seq) and Attention},
	howpublished = {\url{https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html}},
	author = {Voita},
	note = {Accessed: 2022-05-05}
}
@misc{karpathyRNN,
	title = {The Unreasonable Effectiveness of Recurrent Neural Networks},
	howpublished = {\url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}},
	author = {Karpathy},
	note = {Accessed: 2022-05-05}
}

@inproceedings{Meng2019SphericalTE,
	title={Spherical Text Embedding},
	author={Yu Meng and Jiaxin Huang and Guangyuan Wang and Chao Zhang and Honglei Zhuang and Lance M. Kaplan and Jiawei Han},
	booktitle={NeurIPS},
	year={2019},
	eprint={1911.01196},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{10.1162/tacl_a_00257,
	author = {Jawanpuria, Pratik and Balgovind, Arjun and Kunchukuttan, Anoop and Mishra, Bamdev},
	title = "{Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach}",
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {7},
	pages = {107-120},
	year = {2019},
	month = {04},
	abstract = "{We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks.We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting.}",
	issn = {2307-387X},
	doi = {10.1162/tacl_a_00257},
	url = {https://doi.org/10.1162/tacl\_a\_00257},
	eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00257/1923013/tacl\_a\_00257.pdf},
}

@inproceedings{radford2021learning,
	title={Learning transferable visual models from natural language supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={International conference on machine learning},
	pages={8748--8763},
	year={2021},
	organization={PMLR},
	eprint={2103.00020},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@inproceedings{cai2021isotropy,
	title={Isotropy in the Contextual Embedding Space: Clusters and Manifolds},
	author={Xingyu Cai and Jiaji Huang and Yuchen Bian and Kenneth Church},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=xYGNO86OWDH}
}


@inproceedings{rajaee-pilehvar-2021-cluster,
	title = "A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space",
	author = "Rajaee, Sara  and
	Pilehvar, Mohammad Taher",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-short.73",
	doi = "10.18653/v1/2021.acl-short.73",
	pages = "575--584",
	abstract = "The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.",
}

@article{wang2021sentimentconcept,
	author={Wang, Yabing and Huang, Guimin and Li, Jun and Li, Hui and Zhou, Ya and Jiang, Hua},
	journal={IEEE Access}, 
	title={Refined Global Word Embeddings Based on Sentiment Concept for Sentiment Analysis}, 
	year={2021},
	volume={9},
	number={},
	pages={37075-37085},
	doi={10.1109/ACCESS.2021.3062654}}

@inproceedings{heck-etal-2020-trippy,
	title = "{T}rip{P}y: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking",
	author = "Heck, Michael  and
	van Niekerk, Carel  and
	Lubis, Nurul  and
	Geishauser, Christian  and
	Lin, Hsien-Chin  and
	Moresi, Marco  and
	Gasic, Milica",
	booktitle = "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = jul,
	year = "2020",
	address = "1st virtual meeting",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.sigdial-1.4",
	pages = "35--44",
	abstract = "Task-oriented dialog systems rely on dialog state tracking (DST) to monitor the user{'}s goal during the course of an interaction. Multi-domain and open-vocabulary settings complicate the task considerably and demand scalable solutions. In this paper we present a new approach to DST which makes use of various copy mechanisms to fill slots with values. Our model has no need to maintain a list of candidate values. Instead, all values are extracted from the dialog context on-the-fly. A slot is filled by one of three copy mechanisms: (1) Span prediction may extract values directly from the user input; (2) a value may be copied from a system inform memory that keeps track of the system{'}s inform operations (3) a value may be copied over from a different slot that is already contained in the dialog state to resolve coreferences within and across domains. Our approach combines the advantages of span-based slot filling methods with memory methods to avoid the use of value picklists altogether. We argue that our strategy simplifies the DST task while at the same time achieving state of the art performance on various popular evaluation sets including Multiwoz 2.1, where we achieve a joint goal accuracy beyond 55{\%}.",
}

@book {carlsson2022topologicaldataanalysis,
	AUTHOR = {Carlsson, Gunnar and Vejdemo-Johansson, Mikael},
	TITLE = {Topological data analysis with applications},
	PUBLISHER = {Cambridge University Press, Cambridge},
	YEAR = {2022},
	PAGES = {xi+220},
	ISBN = {978-1-108-83865-8},
	MRCLASS = {55N31 (62H30)},
	MRNUMBER = {4346385},
	DOI = {10.1017/9781108975704},
	URL = {https://doi.org/10.1017/9781108975704},
}

@misc{schulman2017proximal,
	title={Proximal Policy Optimization Algorithms}, 
	author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	year={2017},
	eprint={1707.06347},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{ouyang2022training,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	year={2022},
	eprint={2203.02155},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{glaese2022improving,
	title={Improving alignment of dialogue agents via targeted human judgements},
	author={Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
	year={2022},
	eprint={2209.14375},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{Ramesh2022HierarchicalTI,
	title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
	author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
	year={2022},
	eprint={2204.06125},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{radford2022robust,
	title={Robust speech recognition via large-scale weak supervision},
	author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	journal={arXiv preprint arXiv:2212.04356},
	year={2022}
}

@misc{bronstein2021geometric,
	title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
	author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
	year={2021},
	eprint={2104.13478},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{kushnareva-etal-2021-artificial,
	title = "Artificial Text Detection via Examining the Topology of Attention Maps",
	author = "Kushnareva, Laida  and
	Cherniavskii, Daniil  and
	Mikhailov, Vladislav  and
	Artemova, Ekaterina  and
	Barannikov, Serguei  and
	Bernstein, Alexander  and
	Piontkovskaya, Irina  and
	Piontkovski, Dmitri  and
	Burnaev, Evgeny",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.50",
	doi = "10.18653/v1/2021.emnlp-main.50",
	pages = "635--649",
	abstract = "The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10{\%} on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information.",
}

@misc{moor2021topological,
	title={Topological Autoencoders}, 
	author={Michael Moor and Max Horn and Bastian Rieck and Karsten Borgwardt},
	year={2021},
	eprint={1906.00722},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{barannikov2022representation,
	title={Representation Topology Divergence: A Method for Comparing Neural Network Representations}, 
	author={Serguei Barannikov and Ilya Trofimov and Nikita Balabin and Evgeny Burnaev},
	year={2022},
	eprint={2201.00058},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{trofimov2023learning,
	title={Learning Topology-Preserving Data Representations}, 
	author={Ilya Trofimov and Daniil Cherniavskii and Eduard Tulchinskii and Nikita Balabin and Evgeny Burnaev and Serguei Barannikov},
	year={2023},
	eprint={2302.00136},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{perez2022topological,
	title={The Topological BERT: Transforming Attention into Topology for Natural Language Processing}, 
	author={Ilan Perez and Raphael Reinauer},
	year={2022},
	eprint={2206.15195},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{ChatGPT, 
	title={ChatGPT},
	author={OpenAI},
	url={https://chat.openai.com/}, 
	note={\url{https://chat.openai.com/}},
	journal={ChatGPT}, 
	publisher={OpenAI},
	date=2023,
	month=02,
	day=02
}

@misc{openai2023gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{bubeck2023sparks,
	title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
	author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
	year={2023},
	eprint={2303.12712},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{bai2022training,
	title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
	author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
	year={2022},
	eprint={2204.05862},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{lambert2022illustrating,
	author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},
	title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},
	journal = {Hugging Face Blog},
	year = {2022},
	note = {\url{https://huggingface.co/blog/rlhf}},
}