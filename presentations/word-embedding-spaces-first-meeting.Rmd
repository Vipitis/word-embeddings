---
title: "Word embedding spaces"
subtitle: "First session: Introduction"
author: "Benjamin Ruppik"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: yes
    transition: faster
  beamer_presentation: default
  slidy_presentation: default
bibliography: 02_references.bib
header-includes:
- \DeclareMathOperator*{\argmin}{argmin}
- \DeclareMathOperator*{\argmax}{argmax}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Syllabus

* Static word embeddings
    * Frequency based methods, word2vec, GloVe, fastText
* Contextual word embeddings
    * ELMo, **Transformers** and attention, BERT, sentence embeddings, contrastive learning 
* Additional topics
    * Evaluation, geometry of the embedding space, bias, sentiment, multilingual embeddings
* Topological data analysis
    * Hyperbolic embeddings, singularities and topological polysemy

## Motivation

* *Winograd schemas*
  * The city councilmen refused the demonstrators a permit because they feared violence.
  * The city councilmen refused the demonstrators a permit because they advocated violence.
  
  * The trophy doesn’t fit into the brown suitcase because it’s too large.
  * The trophy doesn’t fit into the brown suitcase because it’s too small.
  
## Motivation

* *Garden-path sentences*
  * The old man the boat.
  * The complex houses married and single soldiers and their families.
  * The horse raced past the barn fell.
  
## Methods

"You shall know a word by the company it keeps." (J. R. Firth, 1957)


  
## Applications of word embeddings

* Word-sense induction (WSI) or word-sense discrimination: identification of the senses/meanings of a word
  * output: clustering of contexts of the target word, or a clustering of words related to the target word
* Word-sense disambiguation (WSD): relies on a predefined sense inventory,
and the task is to solve the ambiguity in the context, i.e. identifying which sense of a word is used in a sentence


Part-of-speech tagging

* grammatical tagging: decide which part of speech
(noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection)
in a text/corpus belongs to
* this might depend both on the definition of the word and its context, and in language
a large portion of word-forms are ambiguous
  * Example from wikipedia: "dogs" usually is a plural noun, but can also be a verb as in the sentence
  "The sailor dogs the hatch."
* sub-categories:
  * for nouns, the plural, possessive, and singular forms can be distinguished.
  * "case" (role as subject, object, etc.), grammatical gender, and so on
  * while verbs are marked for tense, aspect, and other things


TODO

## Text classification

* Document classification: spam / not spam
* Review classification: positive / negative
* Sentiment

* single-label classification / multi-label classification

## Generative and Discriminative Models

* Generative models:
  * learn undelying data distribution
  $P(x, y) = P(x | y) \cdot P(y)$
  * prediction: given an input $x$, pick a class with the highest joint probability
  $y = \argmax_{k} P(x | y = k) \cdot P(y = k)$
    * maximum a posteriori (MAP) estimate
  
* Discriminative models:
  * learn the boundaries between classes (i.e. learn how to use the features)
  * prediction: given an input $x$, pick a class with the highest conditions
  probability $y = \argmax_{k} P(y = k | y)$
    * Maximum Likelihood Estimate (MLE) of parameters

TODO: How to do prediction

---

*Bag of Words (BoW) assumption*: word order does not matter

## Static word embeddings

TODO

## Contextual word embeddings

TODO

* Sentence-BERT
  * sentence-pair regression tasks like semantic textual similarity (STS)

## Additional topics

TODO

## Topological data analysis

TODO
