---
title: "Natural Language Processing"
subtitle: "Common tasks and glossary"
author: "Benjamin Ruppik"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## NLP tasks

* Word-sense induction (WSI) or word-sense discrimination: identification of the senses/meanings of a word
  * output: clustering of contexts of the target word, or a clustering of words related to the target word
* Word-sense disambiguation (WSD): relies on a predefined sense inventory,
and the task is to solve the ambiguity in the context, i.e. identifying which sense of a word is used in a sentence

TODO

## Word-sense disambiguation

* WordNet as a reference sense inventory for English language
* WordNet encodes concepts as synonym sets

* SemEval: international word sense disambiguation competition
* approaches: knowledge-based, supervised, and unsupervised

TODO

## Part-of-speech tagging

* grammatical tagging: decide which part of speech
(noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection)
in a text/corpus belongs to
* this might depend both on the definition of the word and its context, and in language
a large portion of word-forms are ambiguous
  * Example from wikipedia: "dogs" usually is a plural noun, but can also be a verb as in the sentence
  "The sailor dogs the hatch."
* sub-categories:
  * for nouns, the plural, possessive, and singular forms can be distinguished.
  * "case" (role as subject, object, etc.), grammatical gender, and so on
  * while verbs are marked for tense, aspect, and other things

TODO

## Hidden Markov Models

* Hidden Markov Models
* Viterbi algorithm
* POS tagging with transformers

## WordNet

* sense inventory

TODO

## Benchmarks

* General Language Understanding Evaluation [(GLUE)](https://gluebenchmark.com/)
 benchmark
* Winograd schema challenge (WSC)
  * "The city councilmen refused the demonstrators a permit because they [feared/advocated] violence."


# Text classification

## Training and Cross-Entropy Loss

* $p^{\ast}$ target probability distribution, $p$ predicted probability distribution
* *cross-entropy loss*: $\operatorname{Loss}(p^{\ast}, p) = - \sum_{i=1}^{K} p_{i}^{\ast} \log(p_{i})$
* minimizing cross-entropy is equivalent to maximizing data likelihood

TODO

## Get vector representations for input text of different length

* Bag of Embeddings (BoE): sum of embeddings
  * Weighted Bag of Embeddings

* Bag of Words (BoW): sum of one-hot vectors

TODO

## Term frequency - inverse document frequency

* word $w$
* text $d$
* corpus $D$

$\operatorname{tf-idf}(w, d, D) = \operatorname{tf}(w, d) \cdot \operatorname{idf}(w, D)
= N(w, d) \cdot \frac{\abs{D}}{\abs{d \in D \mid w \in D}}$

TODO

# Recurrent methods

* RNN
* LSTM
* GRU

TODO

## RNN cell

* input vector
* hidden vector: previous networt state

Problems: vanishing and exploding gradients

TODO

## Recurrent networks for text classification

* Take final state or
* Use multiple layers or
* use Bidirectional architecture

TODO: Explain bidirectional in more detail

# Convolutional (CNN)

## Motivation: Convolution for images

* translation invariance

TODO

## Convolution for text

TODO

## Exercises

* Exploring WordNet
* [Fine-tuning huggingface transformers for POS in French](https://signal.onepointltd.com/post/102gbbr/training-an-nlp-model-with-hugging-face-transformers)
