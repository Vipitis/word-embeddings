---
title: "Natural Language Processing"
subtitle: "Common tasks and glossary"
author: "Benjamin Ruppik"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## NLP tasks

* Word-sense induction (WSI) or word-sense discrimination: identification of the senses/meanings of a word
  * output: clustering of contexts of the target word, or a clustering of words related to the target word
* Word-sense disambiguation (WSD): relies on a predefined sense inventory,
and the task is to solve the ambiguity in the context, i.e. identifying which sense of a word is used in a sentence

TODO

## Word-sense disambiguation

* WordNet as a reference sense inventory for English language
* WordNet encodes concepts as synonym sets

* SemEval: international word sense disambiguation competition
* approaches: knowledge-based, supervised, and unsupervised

TODO

## Part-of-speech tagging

* grammatical tagging: decide which part of speech
(noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection)
in a text/corpus belongs to
* this might depend both on the definition of the word and its context, and in language
a large portion of word-forms are ambiguous
  * Example from wikipedia: "dogs" usually is a plural noun, but can also be a verb as in the sentence
  "The sailor dogs the hatch."
* sub-categories:
  * for nouns, the plural, possessive, and singular forms can be distinguished.
  * "case" (role as subject, object, etc.), grammatical gender, and so on
  * while verbs are marked for tense, aspect, and other things

TODO

## Hidden Markov Models

* Hidden Markov Models
* Viterbi algorithm
* POS tagging with transformers

## WordNet

* sense inventory

TODO

## Benchmarks

* General Language Understanding Evaluation [(GLUE)](https://gluebenchmark.com/)
 benchmark
* Winograd schema challenge (WSC)
  * "The city councilmen refused the demonstrators a permit because they [feared/advocated] violence."


# Text classification

## Training and Cross-Entropy Loss

* $p^{\ast}$ target probability distribution, $p$ predicted probability distribution
* *cross-entropy loss*: $\operatorname{Loss}(p^{\ast}, p) = - \sum_{i=1}^{K} p_{i}^{\ast} \log(p_{i})$
* minimizing cross-entropy is equivalent to maximizing data likelihood

TODO

## Get vector representations for input text of different length

* Bag of Embeddings (BoE): sum of embeddings
  * Weighted Bag of Embeddings

* Bag of Words (BoW): sum of one-hot vectors

TODO

## Term frequency - inverse document frequency

* word $w$
* text $d$
* corpus $D$

$\operatorname{tf-idf}(w, d, D) = \operatorname{tf}(w, d) \cdot \operatorname{idf}(w, D)
= N(w, d) \cdot \frac{\abs{D}}{\abs{d \in D \mid w \in D}}$

TODO

# Recurrent methods

* RNN
* LSTM
* GRU

TODO

## RNN cell

* input vector
* hidden vector: previous networt state

Problems: vanishing and exploding gradients

TODO

## Recurrent networks for text classification

* Take final state *or*
* Use multiple layers *or*
* use Bidirectional architecture

TODO: Explain bidirectional in more detail

# Convolutional (CNN)

## Motivation: Convolution for images

* translation invariance

TODO

## Convolution for text

TODO

## CNN for language modeling

TODO See later (??) (After introducing Language Models?)

# Language Models

* Markov Property: Independence assumption
  * Counting to estimate probabilities does not work well:
  many of the possible fragments do not appear in the corpus
  $\leadsto$ get assigned zero probability
  * Assumption: The probability of a word only depends on a fixed number of preceding words

* smoothing: redistribute probability mass

TODO

## One layer RNNs

TODO

## Multi-layer RNNs

* feed the hidden states from one RNN to the next one
  * lower layers catch local phenomena
  * higher layers catch longer dependencies

TODO

## Generation strategies in Language Models

* Sample tokens from the predicted probability distribution for the next word
  * sample until EOS (End Of Sentence) token is generated
* We want
  * coherence: the generated text should make sense
  * diversity: the model should be able to produce very different samples
  
* Standard sampling: use distributions of model without modification
* Sample with temperature: Change final softmax ba dividing through temperature $\tau$
* Top-$K$ sampling
* Top-$p$ or Nucleus sampling
  * take top-$p$% of the probability mass

TODO

# Evaluating Language Models (and embeddings)

## Cross-Entropy and Perplexity

* a good language model should assign a high probability to real text

* perplexity

TODO


## Tricks for language models

* weight tying:
  * use the same parameters for input and output word embeddings

# Exercises

## Exercises for WSI and WSD

* Exploring WordNet
* [Fine-tuning huggingface transformers for POS in French](https://signal.onepointltd.com/post/102gbbr/training-an-nlp-model-with-hugging-face-transformers)
