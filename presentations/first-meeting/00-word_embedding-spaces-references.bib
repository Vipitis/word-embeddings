@inproceedings {singh2007mapper,
	booktitle = {Eurographics Symposium on Point-Based Graphics},
	editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker},
	title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition}},
	author = {Singh, Gurjeet and Memoli, Facundo and Carlsson, Gunnar},
	year = {2007},
	publisher = {The Eurographics Association},
	ISSN = {1811-7813},
	ISBN = {978-3-905673-51-7},
	DOI = {10.2312/SPBG/SPBG07/091-100}
}

@article{JMLR:v9:vandermaaten08a,
	author  = {Laurens van der Maaten and Geoffrey Hinton},
	title   = {Visualizing Data using t-SNE},
	journal = {Journal of Machine Learning Research},
	year    = {2008},
	volume  = {9},
	number  = {86},
	pages   = {2579-2605},
	url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@book{jurafsky2009speech,
	author={Daniel Jurafsky and James H. Martin},
	title={Speech and Language Processing},
	year={2009},
	ISBN={978-0-13-187321-6},
	publisher={MIT Press}
}

@inproceedings{DBLP:journals/corr/abs-1301-3781,
	author    = {Tom{\'{a}}s Mikolov and
	Kai Chen and
	Greg Corrado and
	Jeffrey Dean},
	editor    = {Yoshua Bengio and
	Yann LeCun},
	title     = {Efficient Estimation of Word Representations in Vector Space},
	booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
	Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
	year      = {2013},
	url       = {http://arxiv.org/abs/1301.3781},
	timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MikolovSCCD13,
	author    = {Tom{\'{a}}s Mikolov and
	Ilya Sutskever and
	Kai Chen and
	Greg Corrado and
	Jeffrey Dean},
	title     = {Distributed Representations of Words and Phrases and their Compositionality},
	journal   = {CoRR},
	volume    = {abs/1310.4546},
	year      = {2013},
	url       = {http://arxiv.org/abs/1310.4546},
	eprinttype = {arXiv},
	eprint    = {1310.4546},
	timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/MikolovSCCD13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/MikolovLS13,
	author    = {Tom{\'{a}}s Mikolov and
	Quoc V. Le and
	Ilya Sutskever},
	title     = {Exploiting Similarities among Languages for Machine Translation},
	journal   = {CoRR},
	volume    = {abs/1309.4168},
	year      = {2013},
	url       = {http://arxiv.org/abs/1309.4168},
	eprinttype = {arXiv},
	eprint    = {1309.4168},
	timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/MikolovLS13.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Rong14,
	author    = {Xin Rong},
	title     = {word2vec Parameter Learning Explained},
	journal   = {CoRR},
	volume    = {abs/1411.2738},
	year      = {2014},
	url       = {http://arxiv.org/abs/1411.2738},
	eprinttype = {arXiv},
	eprint    = {1411.2738},
	timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Rong14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2014_feab05aa,
	author = {Levy, Omer and Goldberg, Yoav},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Neural Word Embedding as Implicit Matrix Factorization},
	url = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf}}

@inproceedings{pennington-etal-2014-glove,
	title = "{G}lo{V}e: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@article{levy-etal-2015-improving,
	title = "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
	author = "Levy, Omer  and
	Goldberg, Yoav  and
	Dagan, Ido",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "3",
	year = "2015",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/Q15-1016",
	doi = "10.1162/tacl_a_00134",
	pages = "211--225",
	abstract = "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.",
}

@article{DBLP:journals/corr/Goldberg15c,
	author    = {Yoav Goldberg},
	title     = {A Primer on Neural Network Models for Natural Language Processing},
	journal   = {CoRR},
	volume    = {abs/1510.00726},
	year      = {2015},
	url       = {http://arxiv.org/abs/1510.00726},
	eprinttype = {arXiv},
	eprint    = {1510.00726},
	timestamp = {Mon, 13 Aug 2018 16:48:41 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/Goldberg15c.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Luong-etal:naacl15:bivec,
        Address = {Denver, United States},
        Author = {Luong, Minh-Thang  and  Pham, Hieu and Manning, Christopher D.},
        Booktitle = {NAACL Workshop on Vector Space Modeling for NLP},
        Title = {Bilingual Word Representations with Monolingual Quality in Mind},
        Year = {2015}
}

@article{DBLP:journals/corr/BolukbasiCZSK16a,
	author    = {Tolga Bolukbasi and
	Kai{-}Wei Chang and
	James Y. Zou and
	Venkatesh Saligrama and
	Adam Kalai},
	title     = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing
	Word Embeddings},
	journal   = {CoRR},
	volume    = {abs/1607.06520},
	year      = {2016},
	url       = {http://arxiv.org/abs/1607.06520},
	eprinttype = {arXiv},
	eprint    = {1607.06520},
	timestamp = {Mon, 13 Aug 2018 16:46:57 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BolukbasiCZSK16a.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hamilton-etal-2016-diachronic,
	title = "Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change",
	author = "Hamilton, William L.  and
	Leskovec, Jure  and
	Jurafsky, Dan",
	booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2016",
	address = "Berlin, Germany",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P16-1141",
	doi = "10.18653/v1/P16-1141",
	pages = "1489--1501",
}

@article{DBLP:journals/corr/BojanowskiGJM16,
	author    = {Piotr Bojanowski and
	Edouard Grave and
	Armand Joulin and
	Tom{\'{a}}s Mikolov},
	title     = {Enriching Word Vectors with Subword Information},
	journal   = {CoRR},
	volume    = {abs/1607.04606},
	year      = {2016},
	url       = {http://arxiv.org/abs/1607.04606},
	eprinttype = {arXiv},
	eprint    = {1607.04606},
	timestamp = {Mon, 28 Dec 2020 11:31:02 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/BojanowskiGJM16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
	author    = {Ashish Vaswani and
	Noam Shazeer and
	Niki Parmar and
	Jakob Uszkoreit and
	Llion Jones and
	Aidan N. Gomez and
	Lukasz Kaiser and
	Illia Polosukhin},
	title     = {Attention Is All You Need},
	journal   = {CoRR},
	volume    = {abs/1706.03762},
	year      = {2017},
	url       = {http://arxiv.org/abs/1706.03762},
	eprinttype = {arXiv},
	eprint    = {1706.03762},
	timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1710-04087,
	author    = {Alexis Conneau and
	Guillaume Lample and
	Marc'Aurelio Ranzato and
	Ludovic Denoyer and
	Herv{\'{e}} J{\'{e}}gou},
	title     = {Word Translation Without Parallel Data},
	journal   = {CoRR},
	volume    = {abs/1710.04087},
	year      = {2017},
	url       = {http://arxiv.org/abs/1710.04087},
	eprinttype = {arXiv},
	eprint    = {1710.04087},
	timestamp = {Mon, 13 Aug 2018 16:48:22 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1710-04087.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mimno-thompson-2017-strange,
	title = "The strange geometry of skip-gram with negative sampling",
	author = "Mimno, David  and
	Thompson, Laure",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D17-1308",
	doi = "10.18653/v1/D17-1308",
	pages = "2873--2878",
	abstract = "Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood. We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors. We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.",
}

@article{DBLP:journals/corr/MuBV17,
	author    = {Jiaqi Mu and
	Suma Bhat and
	Pramod Viswanath},
	title     = {All-but-the-Top: Simple and Effective Postprocessing for Word Representations},
	journal   = {CoRR},
	volume    = {abs/1702.01417},
	year      = {2017},
	url       = {http://arxiv.org/abs/1702.01417},
	eprinttype = {arXiv},
	eprint    = {1702.01417},
	timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MuBV17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yu-etal-2017-refining,
	title = "Refining Word Embeddings for Sentiment Analysis",
	author = "Yu, Liang-Chih  and
	Wang, Jin  and
	Lai, K. Robert  and
	Zhang, Xuejie",
	booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
	month = sep,
	year = "2017",
	address = "Copenhagen, Denmark",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D17-1056",
	doi = "10.18653/v1/D17-1056",
	pages = "534--539",
	abstract = "Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST).",
}

@article{DBLP:journals/corr/NickelK17,
	author    = {Maximilian Nickel and
	Douwe Kiela},
	title     = {Poincar{\'{e}} Embeddings for Learning Hierarchical Representations},
	journal   = {CoRR},
	volume    = {abs/1705.08039},
	year      = {2017},
	url       = {http://arxiv.org/abs/1705.08039},
	eprinttype = {arXiv},
	eprint    = {1705.08039},
	timestamp = {Mon, 13 Aug 2018 16:47:24 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/NickelK17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nakashole-flauger-2018-characterizing,
	title = "Characterizing Departures from Linearity in Word Translation",
	author = "Nakashole, Ndapa  and
	Flauger, Raphael",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-2036",
	doi = "10.18653/v1/P18-2036",
	pages = "221--227",
	abstract = "We investigate the behavior of maps learned by machine translation methods. The maps translate words by projecting between word embedding spaces of different languages. We locally approximate these maps using linear maps, and find that they vary across the word embedding space. This demonstrates that the underlying maps are non-linear. Importantly, we show that the locally linear maps vary by an amount that is tightly correlated with the distance between the neighborhoods on which they are trained. Our results can be used to test non-linear methods, and to drive the design of more accurate maps for word translation.",
}

@article{DBLP:journals/corr/abs-1808-08933,
	author    = {Xilun Chen and
	Claire Cardie},
	title     = {Unsupervised Multilingual Word Embeddings},
	journal   = {CoRR},
	volume    = {abs/1808.08933},
	year      = {2018},
	url       = {http://arxiv.org/abs/1808.08933},
	eprinttype = {arXiv},
	eprint    = {1808.08933},
	timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1808-08933.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{budzianowski-etal-2018-multiwoz,
	title = "{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling",
	author = "Budzianowski, Pawe{\l}  and
	Wen, Tsung-Hsien  and
	Tseng, Bo-Hsiang  and
	Casanueva, I{\~n}igo  and
	Ultes, Stefan  and
	Ramadan, Osman  and
	Ga{\v{s}}i{\'c}, Milica",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = "10",
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1547",
	doi = "10.18653/v1/D18-1547",
	pages = "5016--5026",
	abstract = "Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",
	eprint={1810.00278},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-1908-10084,
	author    = {Nils Reimers and
	Iryna Gurevych},
	title     = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	journal   = {CoRR},
	volume    = {abs/1908.10084},
	year      = {2019},
	url       = {http://arxiv.org/abs/1908.10084},
	eprinttype = {arXiv},
	eprint    = {1908.10084},
	timestamp = {Thu, 26 Nov 2020 12:13:54 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1908-10084.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1904-02239,
	author    = {Valentin Khrulkov and
	Leyla Mirvakhabova and
	Evgeniya Ustinova and
	Ivan V. Oseledets and
	Victor S. Lempitsky},
	title     = {Hyperbolic Image Embeddings},
	journal   = {CoRR},
	volume    = {abs/1904.02239},
	year      = {2019},
	url       = {http://arxiv.org/abs/1904.02239},
	eprinttype = {arXiv},
	eprint    = {1904.02239},
	timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1904-02239.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-06546,
	author    = {Alexandru Tifrea and
	Gary B{\'{e}}cigneul and
	Octavian{-}Eugen Ganea},
	title     = {Poincar{\'{e}} GloVe: Hyperbolic Word Embeddings},
	journal   = {CoRR},
	volume    = {abs/1810.06546},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.06546},
	eprinttype = {arXiv},
	eprint    = {1810.06546},
	timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-06546.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-04313,
	author    = {Bhuwan Dhingra and
	Christopher J. Shallue and
	Mohammad Norouzi and
	Andrew M. Dai and
	George E. Dahl},
	title     = {Embedding Text in Hyperbolic Spaces},
	journal   = {CoRR},
	volume    = {abs/1806.04313},
	year      = {2018},
	url       = {http://arxiv.org/abs/1806.04313},
	eprinttype = {arXiv},
	eprint    = {1806.04313},
	timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1806-04313.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1802-05365,
	author    = {Matthew E. Peters and
	Mark Neumann and
	Mohit Iyyer and
	Matt Gardner and
	Christopher Clark and
	Kenton Lee and
	Luke Zettlemoyer},
	title     = {Deep contextualized word representations},
	journal   = {CoRR},
	volume    = {abs/1802.05365},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.05365},
	eprinttype = {arXiv},
	eprint    = {1802.05365},
	timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
	author    = {Jacob Devlin and
	Ming{-}Wei Chang and
	Kenton Lee and
	Kristina Toutanova},
	title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
	Understanding},
	journal   = {CoRR},
	volume    = {abs/1810.04805},
	year      = {2018},
	url       = {http://arxiv.org/abs/1810.04805},
	eprinttype = {arXiv},
	eprint    = {1810.04805},
	timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-11692,
	author    = {Yinhan Liu and
	Myle Ott and
	Naman Goyal and
	Jingfei Du and
	Mandar Joshi and
	Danqi Chen and
	Omer Levy and
	Mike Lewis and
	Luke Zettlemoyer and
	Veselin Stoyanov},
	title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
	journal   = {CoRR},
	volume    = {abs/1907.11692},
	year      = {2019},
	url       = {http://arxiv.org/abs/1907.11692},
	eprinttype = {arXiv},
	eprint    = {1907.11692},
	timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{eisenstein2019introduction,
	title={Introduction to Natural Language Processing},
	author={Eisenstein, J.},
	isbn={9780262042840},
	lccn={2018059552},
	series={Adaptive Computation and Machine Learning series},
	url={https://books.google.de/books?id=72yuDwAAQBAJ},
	year={2019},
	publisher={MIT Press}
}

@article{DBLP:journals/corr/abs-1901-09813,
	author    = {Carl Allen and
	Timothy M. Hospedales},
	title     = {Analogies Explained: Towards Understanding Word Embeddings},
	journal   = {CoRR},
	volume    = {abs/1901.09813},
	year      = {2019},
	url       = {http://arxiv.org/abs/1901.09813},
	eprinttype = {arXiv},
	eprint    = {1901.09813},
	timestamp = {Sat, 23 Jan 2021 01:12:55 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09813.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{murugan2019introduction,
	title={An Introduction to Topological Data Analysis for Physicists: From LGM to FRBs}, 
	author={Jeff Murugan and Duncan Robertson},
	year={2019},
	eprint={1904.11044},
	archivePrefix={arXiv},
	primaryClass={astro-ph.IM}
}

@inproceedings{ethayarajh-2019-contextual,
	title = "How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings",
	author = "Ethayarajh, Kawin",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1006",
	doi = "10.18653/v1/D19-1006",
	pages = "55--65",
	abstract = "Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5{\%} of the variance in a word{'}s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.",
}

@article{sentiment-aware-word-embedding-emotion-2019, 
	title={Sentiment-Aware Word Embedding for Emotion Classification},
	author={Mao, Xingliang and Chang, Shuai and Shi, Jinjing and Li, Fangfang and Shi, Ronghua},
	volume={9},
	ISSN={2076-3417},
	url={http://dx.doi.org/10.3390/app9071334},
	DOI={10.3390/app9071334},
	number={7},
	journal={Applied Sciences},
	publisher={MDPI AG},
	year={2019},
	pages={1334}
}

@inproceedings{clark-etal-2019-bert,
	title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
	author = "Clark, Kevin  and
	Khandelwal, Urvashi  and
	Levy, Omer  and
	Manning, Christopher D.",
	booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
	month = aug,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-4828",
	doi = "10.18653/v1/W19-4828",
	pages = "276--286",
	abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@inproceedings{zhao-etal-2019-gender,
	title = "Gender Bias in Contextualized Word Embeddings",
	author = "Zhao, Jieyu  and
	Wang, Tianlu  and
	Yatskar, Mark  and
	Cotterell, Ryan  and
	Ordonez, Vicente  and
	Chang, Kai-Wei",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1064",
	doi = "10.18653/v1/N19-1064",
	pages = "629--634",
	abstract = "In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo{'}s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",
}

@inproceedings{kurita-etal-2019-measuring,
	title = "Measuring Bias in Contextualized Word Representations",
	author = "Kurita, Keita  and
	Vyas, Nidhi  and
	Pareek, Ayush  and
	Black, Alan W  and
	Tsvetkov, Yulia",
	booktitle = "Proceedings of the First Workshop on Gender Bias in Natural Language Processing",
	month = aug,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-3823",
	doi = "10.18653/v1/W19-3823",
	pages = "166--172",
	abstract = "Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.",
}

@inproceedings{li-etal-2020-sentence,
	title = "On the Sentence Embeddings from Pre-trained Language Models",
	author = "Li, Bohan  and
	Zhou, Hao  and
	He, Junxian  and
	Wang, Mingxuan  and
	Yang, Yiming  and
	Li, Lei",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.733",
	doi = "10.18653/v1/2020.emnlp-main.733",
	pages = "9119--9130",
	abstract = "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at \url{https://github.com/bohanli/BERT-flow}.",
}

@inproceedings{jakubowski2020topology,
	title={Topology of word embeddings: Singularities reflect polysemy},
	author={Jakubowski, Alexander and Gasic, Milica and Zibrowius, Marcus},
	booktitle={Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics},
	pages={103--113},
	year={2020},
	eprint={2011.09413},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{ravfogel-etal-2020-null,
	title = "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
	author = "Ravfogel, Shauli  and
	Elazar, Yanai  and
	Gonen, Hila  and
	Twiton, Michael  and
	Goldberg, Yoav",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.647",
	doi = "10.18653/v1/2020.acl-main.647",
	pages = "7237--7256",
	abstract = "The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.",
}

@inproceedings{gonen-etal-2020-simple,
	title = "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora",
	author = "Gonen, Hila  and
	Jawahar, Ganesh  and
	Seddah, Djam{\'e}  and
	Goldberg, Yoav",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.51",
	doi = "10.18653/v1/2020.acl-main.51",
	pages = "538--555",
	abstract = "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",
}

@inproceedings{zhu-etal-2020-hypertext,
	title = "{H}yper{T}ext: Endowing {F}ast{T}ext with Hyperbolic Geometry",
	author = "Zhu, Yudong  and
	Zhou, Di  and
	Xiao, Jinghui  and
	Jiang, Xin  and
	Chen, Xiao  and
	Liu, Qun",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.104",
	doi = "10.18653/v1/2020.findings-emnlp.104",
	pages = "1166--1171",
	abstract = "Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchies precisely with limited representation capacity. Considering that hyperbolic space is naturally suitable for modelling tree-like hierarchical data, we propose a new model named HyperText for efficient text classification by endowing FastText with hyperbolic geometry. Empirically, we show that HyperText outperforms FastText on a range of text classification tasks with much reduced parameters.",
}


@misc{reinauer2021persformer,
	title={Persformer: A Transformer Architecture for Topological Machine Learning}, 
	author={Raphael Reinauer and Matteo Caorsi and Nicolas Berkouk},
	year={2021},
	eprint={2112.15210},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2007-01852,
	author    = {Fangxiaoyu Feng and
	Yinfei Yang and
	Daniel Cer and
	Naveen Arivazhagan and
	Wei Wang},
	title     = {Language-agnostic {BERT} Sentence Embedding},
	journal   = {CoRR},
	volume    = {abs/2007.01852},
	year      = {2020},
	url       = {https://arxiv.org/abs/2007.01852},
	eprinttype = {arXiv},
	eprint    = {2007.01852},
	timestamp = {Mon, 06 Jul 2020 15:26:01 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2007-01852.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{guhr-etal-2020-training,
	title = "Training a Broad-Coverage {G}erman Sentiment Classification Model for Dialog Systems",
	author = {Guhr, Oliver  and
	Schumann, Anne-Kathrin  and
	Bahrmann, Frank  and
	B{\"o}hme, Hans Joachim},
	booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
	month = may,
	year = "2020",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2020.lrec-1.202",
	pages = "1627--1632",
	abstract = "This paper describes the training of a general-purpose German sentiment classification model. Sentiment classification is an important aspect of general text analytics. Furthermore, it plays a vital role in dialogue systems and voice interfaces that depend on the ability of the system to pick up and understand emotional signals from user utterances. The presented study outlines how we have collected a new German sentiment corpus and then combined this corpus with existing resources to train a broad-coverage German sentiment model. The resulting data set contains 5.4 million labelled samples. We have used the data to train both, a simple convolutional and a transformer-based classification model and compared the results achieved on various training configurations. The model and the data set will be published along with this paper.",
	language = "English",
	ISBN = "979-10-95546-34-4",
}

@inproceedings{barbieri-etal-2020-tweeteval,
	title = "{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification",
	author = "Barbieri, Francesco  and
	Camacho-Collados, Jose  and
	Espinosa Anke, Luis  and
	Neves, Leonardo",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.148",
	doi = "10.18653/v1/2020.findings-emnlp.148",
	pages = "1644--1650",
	abstract = "The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.",
}

@article{DBLP:journals/corr/abs-2104-03869,
	author    = {Boli Chen and
	Yao Fu and
	Guangwei Xu and
	Pengjun Xie and
	Chuanqi Tan and
	Mosha Chen and
	Liping Jing},
	title     = {Probing {BERT} in Hyperbolic Spaces},
	journal   = {CoRR},
	volume    = {abs/2104.03869},
	year      = {2021},
	url       = {https://arxiv.org/abs/2104.03869},
	eprinttype = {arXiv},
	eprint    = {2104.03869},
	timestamp = {Tue, 13 Apr 2021 16:46:17 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2104-03869.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dey2021computational,
	title={Computational Topology for Data Analysis},
	author={Dey, Tamal K and Wang, Yusu},
	year={2021},
	publisher={Cambridge University Press},
	notes={\url{https://www.cs.purdue.edu/homes/tamaldey/book/CTDAbook/CTDAbook.pdf}}
}

@misc{vanniekerk2021uncertainty,
	title={Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance}, 
	author={Carel van Niekerk and Andrey Malinin and Christian Geishauser and Michael Heck and Hsien-chin Lin and Nurul Lubis and Shutong Feng and Milica Gašić},
	year={2021},
	eprint={2109.04349},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{kumar2021identifying,
	title={Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings}, 
	author={Vaibhav Kumar and Tenzin Singhay Bhotia and Vaibhav Kumar and Tanmoy Chakraborty},
	year={2021},
	eprint={2109.13767},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{lupo2021persistence,
	title={Persistence Steenrod modules}, 
	author={Umberto Lupo and Anibal M. Medina-Mardones and Guillaume Tauzin},
	year={2021},
	eprint={1812.05031},
	archivePrefix={arXiv},
	primaryClass={math.AT}
}

@article{DBLP:journals/corr/abs-2109-04919,
	author    = {Shutong Feng and
	Nurul Lubis and
	Christian Geishauser and
	Hsien{-}Chin Lin and
	Michael Heck and
	Carel van Niekerk and
	Milica Gasic},
	title     = {EmoWOZ: {A} Large-Scale Corpus and Labelling Scheme for Emotion in
	Task-Oriented Dialogue Systems},
	journal   = {CoRR},
	volume    = {abs/2109.04919},
	year      = {2021},
	url       = {https://arxiv.org/abs/2109.04919},
	eprinttype = {arXiv},
	eprint    = {2109.04919},
	timestamp = {Thu, 30 Dec 2021 14:34:09 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2109-04919.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2109-13059,
	author    = {Fangyu Liu and
	Yunlong Jiao and
	Jordan Massiah and
	Emine Yilmaz and
	Serhii Havrylov},
	title     = {Trans-Encoder: Unsupervised sentence-pair modelling through self-
	and mutual-distillations},
	journal   = {CoRR},
	volume    = {abs/2109.13059},
	year      = {2021},
	url       = {https://arxiv.org/abs/2109.13059},
	eprinttype = {arXiv},
	eprint    = {2109.13059},
	timestamp = {Mon, 04 Oct 2021 17:22:25 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2109-13059.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2104-08821,
	author    = {Tianyu Gao and
	Xingcheng Yao and
	Danqi Chen},
	title     = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
	journal   = {CoRR},
	volume    = {abs/2104.08821},
	year      = {2021},
	url       = {https://arxiv.org/abs/2104.08821},
	eprinttype = {arXiv},
	eprint    = {2104.08821},
	timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08821.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dosovitskiy2021an,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	booktitle={International Conference on Learning Representations},
	year={2021},
	url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{kaneko-bollegala-2021-debiasing,
	title = "Debiasing Pre-trained Contextualised Embeddings",
	author = "Kaneko, Masahiro  and
	Bollegala, Danushka",
	booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.107",
	doi = "10.18653/v1/2021.eacl-main.107",
	pages = "1256--1266",
	abstract = "In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",
}

@misc{bronstein2021geometric,
	title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
	author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
	year={2021},
	eprint={2104.13478},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}